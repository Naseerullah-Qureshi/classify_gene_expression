import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, auc, roc_curve, precision_recall_curve
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier

# Load the Down-regulated genes dataset
dataset_path = "D:/brain_dr_-0.7.csv"
dataset = pd.read_csv(dataset_path)

# Separate features and target
X = dataset.drop(columns=['Target'])
y = dataset['Target']

# Use MIFS to select 5 features
selector_mifs = SelectKBest(mutual_info_classif, k=5)
X_selected_mifs = selector_mifs.fit_transform(X, y)

# Get selected feature names
selected_feature_names = X.columns[selector_mifs.get_support()]

# Plot feature importance from MIFS
plt.figure(figsize=(10, 6))
sns.barplot(x=X.columns[selector_mifs.get_support()], y=selector_mifs.scores_[selector_mifs.get_support()])
plt.title("Feature Importance of Selected 5 Features from MIFS")
plt.xticks(rotation=45)
plt.show()

# Use feature importance to select top 2 features from the 5 selected features
importances = selector_mifs.scores_[selector_mifs.get_support()]
top_features_indices = np.argsort(importances)[::-1][:2]
selected_features_final = selected_feature_names[top_features_indices]

# Display selected features based on importance
print("Top 2 Selected Features:")
for feature in selected_features_final:
    print(feature)

# Split data into train and test sets
X_final = X[selected_features_final]
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize classifiers
classifiers = {
    "SVC": SVC(probability=True),
    "KNeighborsClassifier": KNeighborsClassifier(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    "ExtraTreesClassifier": ExtraTreesClassifier(),
    "RandomForestClassifier": RandomForestClassifier(),
    "MLPClassifier": MLPClassifier()
}

# Combine predictions for ROC and Precision-Recall curves
roc_combined = {}
pr_combined = {}
results = {}
for name, clf in classifiers.items():
    clf.fit(X_train_scaled, y_train)
    y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
    roc_combined[name] = (fpr, tpr)
    pr_combined[name] = (precision, recall)
    
    y_pred = clf.predict(X_test_scaled)
    classification_rep = classification_report(y_test, y_pred)
    results[name] = {
        'classification_report': classification_rep
    }
    
    acc = accuracy_score(y_test, y_pred)
    results[name]['accuracy'] = acc

# Plot combined ROC curve
plt.figure(figsize=(10, 6))
for name, (fpr, tpr) in roc_combined.items():
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc(fpr, tpr):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Combined ROC Curve')
plt.legend()
plt.show()

# Plot combined Precision-Recall curve
plt.figure(figsize=(10, 6))
for name, (precision, recall) in pr_combined.items():
    plt.plot(recall, precision, label=f'{name} (AUC = {auc(recall, precision):.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Combined Precision-Recall Curve')
plt.legend()
plt.show()

# Display classification reports
for name, result in results.items():
    print(f"Classifier: {name}")
    print(result['classification_report'])
    print("=" * 50)

# Classification report and bar plot for all classifiers with space between bars
plt.figure(figsize=(12, 6))

bar_width = 0.35  # Width of each bar
bar_positions = np.arange(len(classifiers)) * 0.5  # Adjust spacing between bars

accuracies = [results[name]['accuracy'] for name in classifiers]

for i, (name, clf) in enumerate(classifiers.items()):
    plt.bar(bar_positions[i], accuracies[i], width=bar_width, align='center', label=name)
    plt.text(bar_positions[i], accuracies[i] + 0.005, f'{accuracies[i]:.2f}', ha='center', va='bottom')

plt.title('Classifier Accuracies')
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.xticks(bar_positions, list(classifiers.keys()), rotation=45)
plt.legend()
plt.tight_layout()
plt.show()
